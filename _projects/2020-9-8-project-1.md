---
title: "The Teaching Dimension of Kernel Perceptrons"
collection: projects
type: "Research Fellowship"
permalink: /projects/2020-9-8-project-2
location: "Max-Planck Institute for Software Systems"
date: 2020-06-05
---

Algorithmic machine teaching has been studied under the linear setting where exact
 teaching is possible. However, little is known for teaching nonlinear learners. Here,
 we establish the sample complexity of teaching, aka teaching dimension, for kernel4 ized perceptrons for different families of feature maps. As a warm-up, we show that
the teaching complexity is Θ(d) for the exact teaching of linear perceptrons in R^d,
and Θ(d^k) for kernel perceptron with a polynomial kernel of order k. Furthermore, under certain smooth assumptions on the data distribution, we establish a rigorous bound on the complexity for approximately teaching a Gaussian kernel perceptron.
We provide numerical examples of the optimal (approximate) teaching set under several canonical settings for linear, polynomial and Gaussian kernel perceptrons.

**Under Submission**{:target="_blank"}.
